{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import reader\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.flags\n",
    "\n",
    "flags.DEFINE_string(\"data_path\", None,\n",
    "                    \"Where the training/test data is stored.\")\n",
    "flags.DEFINE_string(\"save_path\", None,\n",
    "                    \"Model output directory.\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    raw_data = reader.ptb_raw_data(FLAGS.data_path)\n",
    "    train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "    init_scale = 0.05\n",
    "    batch_size = 20\n",
    "    num_steps = 35\n",
    "    size = 650\n",
    "    vocab_size = 10000\n",
    "    num_layers = 4\n",
    "    max_grad_norm = 5\n",
    "    max_max_epoch = 39\n",
    "    lr_decay = 0.8\n",
    "    max_epoch = 6\n",
    "    learning_rate = 1.0\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "        # train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "\n",
    "        epoch_size = (len(train_data) // batch_size - 1) // num_steps\n",
    "        input_data, targets = reader.ptb_producer(\n",
    "            train_data, batch_size, num_steps, name=\"TrainInput\")\n",
    "\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            # m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "            def lstm_cell():\n",
    "                # size=650 单个lstm单元中隐藏层的大小\n",
    "                return tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "            with tf.variable_scope(\"RNN\"):\n",
    "                # num_layers 代表lstm的数量\n",
    "                cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [lstm_cell() for _ in range(num_layers)], state_is_tuple=True)\n",
    "                # batch_size=20\n",
    "                _initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "                # vocab_size=1000 size=650\n",
    "                embedding = tf.get_variable(\n",
    "                    \"embedding\", [vocab_size, size], dtype=tf.float32)\n",
    "                inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "                outputs = []\n",
    "                state = _initial_state\n",
    "                for time_step in range(num_steps):\n",
    "                    if time_step > 0:\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "                    (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                    outputs.append(cell_output)\n",
    "\n",
    "            output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])\n",
    "            softmax_w = tf.get_variable(\n",
    "                \"softmax_w\", [size, vocab_size], dtype=tf.float32)\n",
    "            softmax_b = tf.get_variable(\n",
    "                \"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "            logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "            loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                tf.reshape(logits, [-1, 1, vocab_size]),\n",
    "                tf.reshape(targets, [-1, 1]),\n",
    "                tf.ones([batch_size * num_steps, 1], dtype=tf.float32),\n",
    "                average_across_batch=False)\n",
    "            cost_op = tf.reduce_sum(loss) / batch_size\n",
    "            final_state = state\n",
    "            lr = tf.Variable(0.0, trainable=False)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost_op, tvars),\n",
    "                                            max_grad_norm)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            train_op = optimizer.apply_gradients(\n",
    "                zip(grads, tvars),\n",
    "                global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "            new_lr = tf.placeholder(\n",
    "                tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "            lr_update = tf.assign(lr, new_lr)\n",
    "\n",
    "            tf.summary.scalar(\"Training Loss\", cost_op)\n",
    "            tf.summary.scalar(\"Learning Rate\", lr)\n",
    "\n",
    "        sv = tf.train.Supervisor(logdir=\"./train_log\")\n",
    "\n",
    "        with sv.managed_session() as session:\n",
    "            for i in range(max_max_epoch):\n",
    "                lr_decay = lr_decay ** max(i + 1 - max_epoch, 0.0)\n",
    "                session.run(lr_update, feed_dict={\n",
    "                            new_lr: learning_rate * lr_decay})\n",
    "\n",
    "                print(\"Epoch: %d Learning rate: %.3f\" %\n",
    "                        (i + 1, session.run(lr)))\n",
    "                # train_perplexity = run_epoch(session, m, eval_op=m.train_op,\n",
    "                #                             verbose=True)\n",
    "                start_time = time.time()\n",
    "                costs = 0.0\n",
    "                iters = 0\n",
    "                state = session.run(_initial_state)\n",
    "                fetches = {\n",
    "                    \"cost\": cost_op,\n",
    "                    \"final_state\": final_state,\n",
    "                    \"train_op\": train_op\n",
    "                }\n",
    "\n",
    "                for step in range(epoch_size):\n",
    "                    feed_dict = {}\n",
    "                    for i, (c, h) in enumerate(_initial_state):\n",
    "                        feed_dict[c] = state[i].c\n",
    "                        feed_dict[h] = state[i].h\n",
    "                    vals = session.run(fetches, feed_dict)\n",
    "                    cost = vals[\"cost\"]\n",
    "                    state = vals[\"final_state\"]\n",
    "\n",
    "                    costs += cost\n",
    "                    iters += num_steps\n",
    "\n",
    "                    if step % (epoch_size // 10) == 10:\n",
    "                        print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                                (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                                iters * batch_size / (time.time() - start_time)))\n",
    "\n",
    "                print(\"Epoch: %d Train Perplexity: %.3f\" %\n",
    "                        (i + 1, np.exp(costs / iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
