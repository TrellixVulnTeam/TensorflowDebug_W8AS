{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "DATA_PATH = '../data/heart.csv'\n",
    "BATCH_SIZE = 3\n",
    "N_FEATURES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(filenames):\n",
    "    \"\"\" filenames is the list of files you want to read from. \n",
    "    In this case, it contains only heart.csv\n",
    "    \"\"\"\n",
    "    # 1.0\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)  # skip the first line in the file\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    # 2.0\n",
    "    # record_defaults are the default values in case some of our columns are empty\n",
    "    # This is also to tell tensorflow the format of our data (the type of the decode result)\n",
    "    # for this dataset, out of 9 feature columns, \n",
    "    # 8 of them are floats (some are integers, but to make our features homogenous, \n",
    "    # we consider them floats), and 1 is string (at position 5)\n",
    "    # the last column corresponds to the lable is an integer\n",
    "\n",
    "    record_defaults = [[1.0] for _ in range(N_FEATURES)]\n",
    "    record_defaults[4] = ['']\n",
    "    record_defaults.append([1])\n",
    "\n",
    "    # read in the 10 rows of data\n",
    "    # 把读取到的value值解码成特征向量，record_defaults定义解码格式及对应的数据类型\n",
    "    print('record_defaults:', record_defaults)\n",
    "    content = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "#     print('content:', content.run())\n",
    "    # convert the 5th column (present/absent) to the binary value 0 and 1\n",
    "    # TensorFlow函数：tf.where\n",
    "    # refs:https://blog.csdn.net/a_a_ron/article/details/79048446\n",
    "    # if content[4] == tf.constant('Present'):\n",
    "    #     tf.constant(1.0)\n",
    "    #  else:\n",
    "    #     tf.constant(0.0)\n",
    "    condition = tf.equal(content[4], tf.constant('Present'))\n",
    "    content[4] = tf.where(condition, tf.constant(1.0), tf.constant(0.0))\n",
    "\n",
    "    # pack all 9 features into a tensor\n",
    "    features = tf.stack(content[:N_FEATURES])\n",
    "\n",
    "    # assign the last column to label\n",
    "    label = content[-1]\n",
    "\n",
    "    # minimum number elements in the queue after a dequeue, used to ensure \n",
    "    # that the samples are sufficiently mixed\n",
    "    # I think 10 times the BATCH_SIZE is sufficient\n",
    "    min_after_dequeue = 10 * BATCH_SIZE\n",
    "\n",
    "    # the maximum number of elements in the queue\n",
    "    capacity = 20 * BATCH_SIZE\n",
    "    # min_after_dequeue是出队后，队列至少剩下min_after_dequeue个数据，如果队列中的数据不足，则等待插入新数据\n",
    "    # batch_size 队尾取出数据\n",
    "    # capacity 队列容量\n",
    "    # shuffle the data to generate BATCH_SIZE sample pairs\n",
    "    data_batch, label_batch = tf.train.shuffle_batch([features, label], batch_size=BATCH_SIZE,\n",
    "                                                     capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return data_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(data_batch, label_batch):\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for _ in range(10):  # generate 10 batches\n",
    "            features, labels = sess.run([data_batch, label_batch])\n",
    "            print(\"features:\", features)\n",
    "            print('labels:', labels)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_batch, label_batch = batch_generator([DATA_PATH])\n",
    "    generate_batches(data_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_defaults: [[1.0], [1.0], [1.0], [1.0], [''], [1.0], [1.0], [1.0], [1.0], [1]]\n",
      "features: [[120.     7.5   15.33  22.     0.    60.    25.31  34.49  49.  ]\n",
      " [118.     6.     9.65  33.91   0.    60.    38.8    0.    48.  ]\n",
      " [117.     1.53   2.44  28.95   1.    35.    25.89  30.03  46.  ]]\n",
      "features: [[144.     4.09   5.55  31.4    1.    60.    29.43   5.55  56.  ]\n",
      " [134.    13.6    3.5   27.78   1.    60.    25.99  57.34  49.  ]\n",
      " [132.     6.2    6.47  36.21   1.    62.    30.77  14.14  45.  ]]\n",
      "features: [[132.     7.9    2.85  26.5    1.    51.    26.16  25.71  44.  ]\n",
      " [160.    12.     5.73  23.11   1.    49.    25.3   97.2   52.  ]\n",
      " [126.     8.75   6.53  34.02   0.    49.    30.25   0.    41.  ]]\n",
      "features: [[138.     0.6    3.81  28.66   0.    54.    28.7    1.46  58.  ]\n",
      " [146.    10.5    8.29  35.36   1.    78.    32.73  13.89  53.  ]\n",
      " [118.     0.28   5.8   33.7    1.    60.    30.98   0.    41.  ]]\n",
      "features: [[114.     4.08   4.59  14.6    1.    62.    23.11   6.72  58.  ]\n",
      " [120.     0.     1.07  16.02   0.    47.    22.15   0.    15.  ]\n",
      " [132.     0.     1.87  17.21   0.    49.    23.63   0.97  15.  ]]\n",
      "features: [[112.     9.65   2.29  17.2    1.    54.    23.53   0.68  53.  ]\n",
      " [140.     3.9    7.32  25.05   0.    47.    27.36  36.77  32.  ]\n",
      " [136.    11.2    5.81  31.85   1.    75.    27.68  22.94  58.  ]]\n",
      "features: [[162.     7.4    8.55  24.65   1.    64.    25.71   5.86  58.  ]\n",
      " [124.    14.     6.23  35.96   1.    45.    30.09   0.    59.  ]\n",
      " [124.     4.    12.42  31.29   1.    54.    23.23   2.06  42.  ]]\n",
      "features: [[130.     0.     2.82  19.63   1.    70.    24.86   0.    29.  ]\n",
      " [142.     4.05   3.38  16.2    0.    59.    20.81   2.62  38.  ]\n",
      " [152.     0.9    9.12  30.23   0.    56.    28.64   0.37  42.  ]]\n",
      "features: [[148.     5.5    7.1   25.31   0.    56.    29.84   3.6   48.  ]\n",
      " [122.     6.6    5.58  35.95   1.    53.    28.07  12.55  59.  ]\n",
      " [134.     2.5    3.66  30.9    0.    52.    27.19  23.66  49.  ]]\n",
      "features: [[1.0300e+02 3.0000e-02 4.2100e+00 1.8960e+01 0.0000e+00 4.8000e+01\n",
      "  2.2940e+01 2.6200e+00 1.8000e+01]\n",
      " [1.5200e+02 5.9900e+00 7.9900e+00 3.2480e+01 0.0000e+00 4.5000e+01\n",
      "  2.6570e+01 1.0032e+02 4.8000e+01]\n",
      " [1.3600e+02 2.5200e+00 3.9500e+00 2.5630e+01 0.0000e+00 5.1000e+01\n",
      "  2.1860e+01 0.0000e+00 4.5000e+01]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
