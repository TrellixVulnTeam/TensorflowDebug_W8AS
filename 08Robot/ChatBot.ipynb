{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"--lines--\"\n",
    "head ./data/cornell\\ movie-dialogs\\ corpus/movie_lines.txt\n",
    "\n",
    "echo \"--conversations--\"\n",
    "head ./data/cornell\\ movie-dialogs\\ corpus/movie_conversations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.prepare_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed\\\\test_ids.enc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7ddbb4d63e76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_buckets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_buckets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_buckets_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtest_buckets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_buckets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_buckets_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_buckets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-7ddbb4d63e76>\u001b[0m in \u001b[0;36m_get_buckets\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mchoose\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0mlater\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtest_buckets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_ids.enc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_ids.dec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mdata_buckets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_ids.enc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_ids.dec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     train_bucket_sizes = [len(data_buckets[b])\n",
      "\u001b[1;32mE:\\OctaveProject\\TensorflowDebug\\08Robot\\data.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(enc_filename, dec_filename, max_training_size)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_training_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[0mencode_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPROCESSED_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[0mdecode_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPROCESSED_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[0mencode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed\\\\test_ids.enc'"
     ]
    }
   ],
   "source": [
    "def _get_buckets():\n",
    "    \"\"\" Load the dataset into buckets based on their lengths.\n",
    "    train_buckets_scale is the inverval that'll help us \n",
    "    choose a random bucket later on.\n",
    "    \"\"\"\n",
    "    test_buckets = data.load_data('test_ids.enc', 'test_ids.dec')\n",
    "    data_buckets = data.load_data('train_ids.enc', 'train_ids.dec')\n",
    "    train_bucket_sizes = [len(data_buckets[b])\n",
    "                          for b in range(len(config.BUCKETS))]\n",
    "    print(\"Number of samples in each bucket:\\n\", train_bucket_sizes)\n",
    "    train_total_size = sum(train_bucket_sizes)\n",
    "    # list of increasing numbers from 0 to 1 that we'll use to select a bucket.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in range(len(train_bucket_sizes))]\n",
    "    print(\"Bucket scale:\\n\", train_buckets_scale)\n",
    "    return test_buckets, data_buckets, train_buckets_scale\n",
    "\n",
    "test_buckets, data_buckets, train_buckets_scale = _get_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.BUCKETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_buckets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_id = 0\n",
    "\n",
    "encoder_inputs, decoder_inputs, decoder_masks = data.get_batch(\n",
    "                data_buckets[bucket_id], bucket_id, batch_size=config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoder_inputs)\n",
    "print(len(decoder_inputs))\n",
    "\n",
    "print(config.BUCKETS[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "encoder_length_tensor = tf.placeholder(tf.int32, shape=(), name='encoder_len')\n",
    "encoder_inputs_tensor = tf.placeholder(tf.int32,\n",
    "                                       shape=[config.BUCKETS[0][0], config.BATCH_SIZE],\n",
    "                                       name='encoder_inputs')\n",
    "decoder_length_tensor = tf.placeholder(tf.int32, shape=(), name='decoder_len')\n",
    "decoder_inputs_tensor = tf.placeholder(tf.int32,\n",
    "                                       shape=[config.BUCKETS[0][1], config.BATCH_SIZE],\n",
    "                                       name='decoder_inputs')\n",
    "\n",
    "with tf.variable_scope('encoder') as scope:\n",
    "    scope = tf.get_variable_scope()\n",
    "    scope.set_initializer(tf.random_uniform_initializer(-0.1,0.1))\n",
    "\n",
    "    W = tf.get_variable(\n",
    "        name=\"W\",\n",
    "        shape=[config.ENC_VOCAB, config.HIDDEN_SIZE],\n",
    "        initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "\n",
    "    source_embedded = tf.nn.embedding_lookup(W, encoder_inputs_tensor)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_units=config.HIDDEN_SIZE)\n",
    "\n",
    "    enc_outputs, enc_final_state = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                                     inputs=source_embedded,\n",
    "                                                     time_major=True,\n",
    "                                                     dtype=tf.float32)\n",
    "\n",
    "CONTEXT_SIZE = 256\n",
    "with tf.variable_scope('decoder') as scope:\n",
    "    scope = tf.get_variable_scope()\n",
    "    scope.set_initializer(tf.random_uniform_initializer(-0.1,0.1))\n",
    "    \n",
    "    W = tf.get_variable(\n",
    "        name=\"W\",\n",
    "        shape=[config.DEC_VOCAB, config.HIDDEN_SIZE],\n",
    "        initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "    target_embedded = tf.nn.embedding_lookup(W, decoder_inputs_tensor)\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_units=config.HIDDEN_SIZE)\n",
    "    \n",
    "    def condition(time, all_outputs, inputs, states):\n",
    "        return time < decoder_length_tensor\n",
    "    \n",
    "    def body(time, all_outputs, inputs, states):\n",
    "        dec_outputs, dec_state = cell(inputs=inputs, state=states)\n",
    "        \n",
    "        # calculate attention\n",
    "        att_keys = tf.contrib.layers.fully_connected(inputs=enc_outputs, num_outputs=CONTEXT_SIZE, activation_fn=None)\n",
    "        att_query = tf.contrib.layers.fully_connected(inputs=dec_outputs, num_outputs=CONTEXT_SIZE, activation_fn=None)\n",
    "        scores = tf.reduce_sum(att_keys * tf.expand_dims(att_query, 0), [2])\n",
    "        scores_normalized = tf.nn.softmax(scores, dim=0)\n",
    "        context = tf.expand_dims(scores_normalized, 2) * enc_outputs\n",
    "        context = tf.reduce_sum(context, [0], name=\"context\")\n",
    "        print(dec_outputs.get_shape())\n",
    "        print(att_keys.get_shape())\n",
    "        print(att_query.get_shape())\n",
    "        print(scores.get_shape())\n",
    "        print(scores_normalized.get_shape())\n",
    "        print(context.get_shape())\n",
    "        \n",
    "        projection_input = tf.concat([dec_outputs, context], 1)\n",
    "        print(\"projection_input:\", projection_input.get_shape())\n",
    "        output_logits = tf.contrib.layers.fully_connected(inputs=projection_input, num_outputs=config.DEC_VOCAB, activation_fn=None)\n",
    "        print(\"output_logits:\", output_logits.get_shape())\n",
    "        all_outputs = all_outputs.write(time, output_logits)\n",
    "        \n",
    "        next_input = tf.concat([target_embedded[time], context], 1)\n",
    "        \n",
    "        return (time + 1, all_outputs, next_input, dec_state)\n",
    "    \n",
    "    output_ta = tensor_array_ops.TensorArray(dtype=tf.float32,\n",
    "                                             size=0,\n",
    "                                             dynamic_size=True,\n",
    "                                             element_shape=(config.BATCH_SIZE, config.DEC_VOCAB))\n",
    "    \n",
    "    init_inputs = tf.concat([target_embedded[0], tf.zeros(dtype=tf.float32, shape=(config.BATCH_SIZE, CONTEXT_SIZE))], 1)\n",
    "    print(\"init_inputs:\", init_inputs.get_shape())\n",
    "    \n",
    "    res = control_flow_ops.while_loop(\n",
    "        condition,\n",
    "        body,\n",
    "        loop_vars=[0, output_ta, init_inputs, enc_final_state],        \n",
    "    )\n",
    "    final_outputs = res[1].stack()\n",
    "    final_state = res[3]\n",
    "    \n",
    "with tf.variable_scope('loss') as scope:\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=final_outputs, labels=decoder_inputs_tensor)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_id = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for it in range(1000):\n",
    "        encoder_inputs, decoder_inputs, decoder_masks = data.get_batch(\n",
    "                    data_buckets[bucket_id], bucket_id, batch_size=config.BATCH_SIZE)\n",
    "\n",
    "        res_loss, _ = sess.run([loss, train_op],\n",
    "                 feed_dict={encoder_inputs_tensor: encoder_inputs,\n",
    "                            decoder_inputs_tensor: decoder_inputs,\n",
    "                            decoder_length_tensor: 10})\n",
    "        print(\"Iteration {} - loss:{}\".format(it, res_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add perplexity calculation\n",
    "# add visualization\n",
    "# running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(map(np.sum, decoder_masks)))\n",
    "print(sum(map(len, decoder_masks)))\n",
    "print(len(data_buckets[bucket_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
